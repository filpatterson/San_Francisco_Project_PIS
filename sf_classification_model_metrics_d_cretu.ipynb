{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sf_classification_model_metrics_d_cretu.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "951e67ffb7f3bd0913ac5cd4eb4e94b70128659a1c74ea9a34ac16faced73bb7"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "iPHCreqCIV6p",
        "outputId": "7ada8681-2303-4151-b223-47537dcd097c"
      },
      "source": [
        "# section for uploading file with training data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f2837c6a-2490-478e-bcb4-3be3b7525eeb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f2837c6a-2490-478e-bcb4-3be3b7525eeb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test.csv.zip to test.csv.zip\n",
            "Saving train.csv.zip to train.csv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoMc7g98HPL-",
        "outputId": "9e473a3e-1512-4d97-fb3f-5c443dc6b957"
      },
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.0.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n",
            "Installing collected packages: ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.1\n",
            "time: 769 µs (started: 2021-08-04 08:13:29 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q47jPnAxmaW1"
      },
      "source": [
        "def read_binary_file(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJH3Jyjj6jBM",
        "outputId": "dcd68f8b-cfdb-4c6d-8461-9cff69acede3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "import os\n",
        "\n",
        "# take information from dataframes\n",
        "train_df = None\n",
        "test_df = None\n",
        "\n",
        "for file in os.listdir():\n",
        "    if file == 'train.csv.zip':\n",
        "        train_df = pd.read_csv(BytesIO(read_binary_file(file)), compression='zip')\n",
        "    elif file == 'test.csv.zip':\n",
        "        test_df = pd.read_csv(BytesIO(read_binary_file(file)), compression='zip')\n",
        "\n",
        "# print shape of data to make sure that it was read\n",
        "print(train_df.shape)\n",
        "print(test_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(878049, 9)\n",
            "(884262, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HMKlJDl6jBQ"
      },
      "source": [
        "# lowest left corner point\n",
        "low_x_threshold = -122.52\n",
        "low_y_threshold =  36.65\n",
        "\n",
        "# highest right corner point\n",
        "high_x_threshold = -122.36\n",
        "high_y_threshold =  40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_dERSR46jBQ"
      },
      "source": [
        "# Data transformation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbQBmu0x6jBR"
      },
      "source": [
        "def get_harmonic_tuple(value, period=24):\n",
        "    \"\"\"\n",
        "    remaps cyclical data from line axis to the circular axis ->\n",
        "    important for correct data interpretation by regression\n",
        "    \"\"\"\n",
        "    value *= 2 * np.pi / period\n",
        "    return np.cos(value), np.sin(value)\n",
        "\n",
        "\n",
        "def get_outlier_removed_col(df: pd.DataFrame, column, up_threshold, low_threshold):\n",
        "    \"\"\"\n",
        "    return dataframe where column is filtered from outliers\n",
        "    by removing elements that are not in given interval\n",
        "    \"\"\"\n",
        "    return df[(df[column] < up_threshold) & (df[column] > low_threshold)]\n",
        "\n",
        "\n",
        "def get_count_table(df: pd.DataFrame, category, time):\n",
        "    \"\"\"\n",
        "    return dataframe of counting elements by groups in pair with time categories\n",
        "    \"\"\"\n",
        "    count_df = df\n",
        "    count_df[\"Count\"] = 1\n",
        "    count_df = count_df[[category, time, 'Count']]\n",
        "    count_df = count_df.groupby([category, time]).agg('sum')\n",
        "    count_df = count_df.reset_index()\n",
        "    return count_df\n",
        "\n",
        "\n",
        "def get_cols_names_below_threshold(df: pd.DataFrame, threshold):\n",
        "    \"\"\"\n",
        "    Return array of columns that are below defined threshold\n",
        "    \"\"\"\n",
        "    categories_below_threshold = df[df[\"Count\"] < threshold]['Category'].unique()\n",
        "    return categories_below_threshold\n",
        "\n",
        "\n",
        "def get_count_table_by_street(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Form new dataframe that shows crime frequency depending on street or intersection by categories\n",
        "    \"\"\"\n",
        "    local_df = pd.DataFrame({})\n",
        "    for elem in df['Street'].unique():\n",
        "        if elem != 0:\n",
        "            count = df[df['Street'] == elem]['Category'].value_counts()\n",
        "            local_df = pd.concat([local_df, pd.Series(count, name=elem)], axis=1)\n",
        "            \n",
        "    return local_df\n",
        "\n",
        "\n",
        "def get_nan_records(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    get all records that have NaN values in any column\n",
        "    \"\"\"\n",
        "    return df[df.isnull().any(axis=1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P809VI56jBS"
      },
      "source": [
        "def make_streets_intersections_cols(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Form two new columns called 'street' and 'intersection' that take address by reges from\n",
        "    specified column\n",
        "    \"\"\"\n",
        "    \n",
        "    intersection = df.Address.str.extract(r'(\\w+\\s\\w+\\s[/]\\s\\w+\\s\\w+)').fillna(' ')\n",
        "    street       = df.Address.str.extract(r'\\d+\\s\\w+\\s\\w+\\s(\\w+\\s\\w+)').fillna(' ')\n",
        "\n",
        "    intersection = intersection.rename(columns = {0 : 'Intersection'})\n",
        "    street       = street.rename(columns = {0 : 'Street'})\n",
        "\n",
        "    df = pd.concat([df, street, intersection], axis=1)\n",
        "    return df\n",
        "\n",
        "\n",
        "def make_time_cols(df: pd.DataFrame, timestamp_column: str):\n",
        "    \"\"\"\n",
        "    return dataframe with time columns formed from string-formatted \n",
        "    timestamps\n",
        "    \"\"\"\n",
        "    df[timestamp_column] = pd.to_datetime(df[timestamp_column])\n",
        "    df[\"Year\"]           = df[timestamp_column].dt.year\n",
        "    df[\"Month\"]          = df[timestamp_column].dt.month\n",
        "    df[\"Day\"]            = df[timestamp_column].dt.date\n",
        "    df[\"DayOfYear\"]      = df[timestamp_column].dt.dayofyear\n",
        "    df[\"Hour\"]           = df[timestamp_column].dt.hour\n",
        "    df[\"Minute\"]         = df[timestamp_column].dt.minute\n",
        "    return df\n",
        "\n",
        "\n",
        "def make_weekday_to_num(df: pd.DataFrame, column: str):\n",
        "    \"\"\"\n",
        "    return column where string-type weekdays will be replaced\n",
        "    by numerical values\n",
        "    \"\"\"\n",
        "    week_day_mapper = {\n",
        "        'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, 'Friday': 4,\n",
        "        'Saturday': 5, 'Sunday': 6,\n",
        "    }\n",
        "\n",
        "    df[\"weekdayNumerical\"] = df[column].map(week_day_mapper).astype(\"int64\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def make_address_encoding_col(df: pd.DataFrame, delimeter: str, column):\n",
        "    \"\"\"\n",
        "    Perform address encoding, where street is 1 and intersection is 0\n",
        "    \"\"\"\n",
        "    address = df[column].apply(lambda record: any([delimeter in record]))\n",
        "    df['address_encoded'] = np.fromiter(address, dtype=bool).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def make_seasons_col(df: pd.DataFrame, column):\n",
        "    \"\"\"\n",
        "    Form new column called 'seasons' depending on month\n",
        "    \"\"\"\n",
        "    df['Season'] = df[column]\n",
        "    df['Season'] = df['Season'].map({1 : 1, 2 : 1, 3 : 2, 4 : 2, 5 : 2, \n",
        "                                     6 : 3, 7 : 3, 8 : 3, 9 : 4, 10: 4, 11: 4, 12: 1\n",
        "    })\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfHs742G6jBT"
      },
      "source": [
        "def get_true_pred_perc(predictions, answers):\n",
        "    \"\"\"\n",
        "    get percentage value of true predictions\n",
        "    \"\"\"\n",
        "    collisions = 0\n",
        "    for index in range(len(answers)):\n",
        "        if answers[index] == predictions[index]:\n",
        "            collisions += 1\n",
        "\n",
        "    return collisions * 100/len(answers)\n",
        "\n",
        "def make_submission_csv(data_to_save, columns, filename='submission_cretu.csv'):\n",
        "    \"\"\"\n",
        "    get percentage of true predictions\n",
        "    \"\"\"\n",
        "    submission_dataframe = pd.DataFrame(data=data_to_save, columns=columns)\n",
        "    submission_dataframe['Id'] = test_df['Id'].astype('int32')\n",
        "    submission_dataframe.to_csv(filename, index=False)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rl0JU4a6jBT"
      },
      "source": [
        "# Data preparation and transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEjQ8EOX6jBT"
      },
      "source": [
        "Basing on most of classification results, weather data is not helping with rising accuracy of the model. Categorical data also does not rise classification accuracy. Data that shows best results - numerical one, combining elements of int and float columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmBJGFCN6jBU"
      },
      "source": [
        "# remove locational outliers\n",
        "train_df = get_outlier_removed_col(train_df, 'Y', \n",
        "                up_threshold=high_y_threshold, low_threshold=low_y_threshold)\n",
        "train_df = get_outlier_removed_col(train_df, 'X', \n",
        "                up_threshold=high_x_threshold, low_threshold=low_x_threshold)\n",
        "\n",
        "# remove data duplicates\n",
        "train_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# remove unnecessary columns\n",
        "train_df.drop(columns=['Resolution', 'Descript'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw1gZODf6jBU"
      },
      "source": [
        "# form time columns extracted from string-formatted timestamps\n",
        "train_df = make_time_cols(train_df, 'Dates')\n",
        "test_df =  make_time_cols(test_df,  'Dates')\n",
        "\n",
        "# make new column that transforms weekdays from string-formatted records in numerical ones\n",
        "train_df = make_weekday_to_num(train_df, 'DayOfWeek')\n",
        "test_df = make_weekday_to_num(test_df, 'DayOfWeek')\n",
        "\n",
        "# form seasons basing on months\n",
        "train_df = make_seasons_col(train_df, \"Month\")\n",
        "test_df = make_seasons_col(test_df, \"Month\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN_OAg6K6jBV"
      },
      "source": [
        "# make harmonic variables that will be necessary for performing effective analysis of periodic features\n",
        "train_df['harmonicHourX'], train_df['harmonicHourY'] = get_harmonic_tuple(train_df['Hour'])\n",
        "train_df['harmonicMinuteX'], train_df['harmonicMinuteY'] = get_harmonic_tuple(train_df['Minute'])\n",
        "train_df['harmonicWeekdayX'], train_df['harmonicWeekdayY'] = get_harmonic_tuple(train_df['weekdayNumerical'])\n",
        "train_df['harmonicDayX'], train_df['harmonicDayY'] = get_harmonic_tuple(train_df['DayOfYear'])\n",
        "train_df['harmonicMonthX'], train_df['harmonicMonthY'] = get_harmonic_tuple(train_df['Month'])\n",
        "\n",
        "test_df['harmonicHourX'], test_df['harmonicHourY'] = get_harmonic_tuple(test_df['Hour'])\n",
        "test_df['harmonicMinuteX'], test_df['harmonicMinuteY'] = get_harmonic_tuple(test_df['Minute'])\n",
        "test_df['harmonicWeekdayX'], test_df['harmonicWeekdayY'] = get_harmonic_tuple(test_df['weekdayNumerical'])\n",
        "test_df['harmonicDayX'], test_df['harmonicDayY'] = get_harmonic_tuple(test_df['DayOfYear'])\n",
        "test_df['harmonicMonthX'], test_df['harmonicMonthY'] = get_harmonic_tuple(test_df['Month'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLAGQqnC6jBV"
      },
      "source": [
        "# generate new column with encoded address by values 0 and 1\n",
        "train_df = make_address_encoding_col(train_df, '/', 'Address')\n",
        "test_df = make_address_encoding_col(test_df, '/', 'Address')\n",
        "\n",
        "# generate new column with extracted streets and intersections\n",
        "train_df = make_streets_intersections_cols(train_df)\n",
        "test_df = make_streets_intersections_cols(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVwnrbQa6jBX"
      },
      "source": [
        "# Column specifications by type, information (for models fit_transorm)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyGpDHZw6jBX"
      },
      "source": [
        "# columns for training and testing info\n",
        "int_columns =   ['Month', 'DayOfYear', 'Hour', 'Minute', 'weekdayNumerical', 'address_encoded', 'Season', \n",
        "                 'Year']\n",
        "float_columns = ['X', 'Y', 'harmonicHourX', 'harmonicHourY', 'harmonicWeekdayX', 'harmonicWeekdayY', \n",
        "                 'harmonicDayX', 'harmonicDayY', 'harmonicMonthX', 'harmonicMonthY', 'harmonicMinuteX', \n",
        "                 'harmonicMinuteY']\n",
        "categorical_columns = ['DayOfWeek', 'PdDistrict', 'Address', 'Street', 'Intersection']\n",
        "numerical_columns =   int_columns + float_columns\n",
        "\n",
        "\n",
        "# columns that were filtered from 'bad' by authors opinion\n",
        "clean_int_columns = ['address_encoded', 'Season', 'Year']\n",
        "clean_float_columns = ['X', 'Y', 'harmonicHourX', 'harmonicHourY', 'harmonicWeekdayX', 'harmonicWeekdayY', \n",
        "                       'harmonicDayX', 'harmonicDayY', 'harmonicMonthX', 'harmonicMonthY', 'harmonicMinuteX', \n",
        "                       'harmonicMinuteY']\n",
        "clean_categorical_columns = ['PdDistrict', 'Street', 'Intersection']\n",
        "clean_numerical_columns =   clean_int_columns + clean_float_columns\n",
        "\n",
        "# what is the target column for classification\n",
        "target = 'Category'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hocVRJ6J6jBX"
      },
      "source": [
        "# Check data integrity and validate transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x31Mi3Hf6jBY"
      },
      "source": [
        "**Заметка для себя**: смотри внимательно на последовательность преобразований данных в ячейке, один раз попал на косяк неправильного преобразования сезонов, потому что трансформация не успела еще сформировать месяца, на основании которых определяется сезон"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY0s44_y6jBZ",
        "outputId": "5fbc71cc-8029-4421-bb21-809c2824d365"
      },
      "source": [
        "# show all records that have NaN values in any column, if dataframes are empty, then there\n",
        "# are no NaN values in dataset\n",
        "print('\\n\\ttraining df:\\n' +                    str(get_nan_records(train_df)))\n",
        "print('\\n\\ttest df:\\n' +                        str(get_nan_records(test_df)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\ttraining df:\n",
            "Empty DataFrame\n",
            "Columns: [Dates, Category, DayOfWeek, PdDistrict, Address, X, Y, Year, Month, Day, DayOfYear, Hour, Minute, weekdayNumerical, Season, harmonicHourX, harmonicHourY, harmonicMinuteX, harmonicMinuteY, harmonicWeekdayX, harmonicWeekdayY, harmonicDayX, harmonicDayY, harmonicMonthX, harmonicMonthY, address_encoded, Street, Intersection]\n",
            "Index: []\n",
            "\n",
            "\ttest df:\n",
            "Empty DataFrame\n",
            "Columns: [Id, Dates, DayOfWeek, PdDistrict, Address, X, Y]\n",
            "Index: []\n",
            "time: 640 ms (started: 2021-08-03 15:23:33 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gjOlb9V6jBZ"
      },
      "source": [
        "# Training models and results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAA2XKvj-1CK"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "import lightgbm as lgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzCTbjQY6jBa"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBrA8DG16jBa"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#  set all scalers and transformers \n",
        "#  note: if you want ordinal encoder to be able to encode unknown classes write in constructor:\n",
        "# \"handle_unknown='use_encoded_value', unknown_value=-1\"\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "standard_scaler = StandardScaler()\n",
        "minmax = MinMaxScaler()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0795k1YM6jBa"
      },
      "source": [
        "## LR numerical data, minmaxXY, standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhNJYMsJWCYd"
      },
      "source": [
        "**THIS ONE SHOWS THE BEST LOG LOSS RESULT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xsV5w7j6jBa",
        "outputId": "d51b43df-16a4-47db-fe63-4a269a07fc03"
      },
      "source": [
        "log_model = LogisticRegression(multi_class='multinomial', solver='saga')\n",
        "\n",
        "# make data standardization\n",
        "x = train_df[numerical_columns]\n",
        "x[['X', 'Y']] = minmax.fit_transform(x[['X', 'Y']])\n",
        "x = standard_scaler.fit_transform(x)\n",
        "\n",
        "y = train_df[target]\n",
        "y = np.array(y)\n",
        "y = np.reshape(y, (-1, 1))\n",
        "y = ordinal_encoder.fit_transform(y)\n",
        "\n",
        "# split data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10, stratify=y)\n",
        "log_model.fit(x_train, y_train)\n",
        "\n",
        "# show percentage of correct results\n",
        "print('>>> log loss coefficient = ' + str(log_loss(y_test, log_model.predict_proba(x_test))))\n",
        "# >>> log loss coefficient = 2.522534169330668"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> log loss coefficient = 2.522534169330668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRKBVl7n6jBb"
      },
      "source": [
        "## LR numerical + categorical, minmaxXY, standardization, ordinal encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMZOqM4s6jBb",
        "outputId": "3146a68c-edb9-44e5-8a7c-0c59e9dbc319"
      },
      "source": [
        "log_cat_model = LogisticRegression(multi_class='multinomial', solver='saga')\n",
        "\n",
        "# make numerical data standardization and categorical data ordinal encoding\n",
        "x_num = train_df[numerical_columns]\n",
        "x_num[['X', 'Y']] = minmax.fit_transform(x_num[['X', 'Y']])\n",
        "x_num = standard_scaler.fit_transform(x_num)\n",
        "x_cat = train_df[categorical_columns]\n",
        "x_cat = ordinal_encoder.fit_transform(x_cat)\n",
        "x = np.concatenate((x_num, x_cat), axis=1)\n",
        "\n",
        "y = train_df[target]\n",
        "y = np.array(y)\n",
        "y = np.reshape(y, (-1, 1))\n",
        "y = ordinal_encoder.fit_transform(y)\n",
        "\n",
        "# split data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10, stratify=y)\n",
        "log_cat_model.fit(x_train, y_train)\n",
        "\n",
        "# show percentage of correct results\n",
        "print('>>> log loss coefficient = ' + str(log_loss(y_test, log_cat_model.predict_proba(x_test))))\n",
        "# >>> log loss coefficient = 2.6964637018643476"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> log loss coefficient = 2.6964637018643476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04v64UzfOQCw"
      },
      "source": [
        "## LR numerical, minmaxXY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTjH4HSdOVYB",
        "outputId": "47358d7e-640e-4a5b-cb48-9b03acaa604e"
      },
      "source": [
        "log_cat_model = LogisticRegression(multi_class='multinomial', solver='saga')\n",
        "\n",
        "# make numerical data standardization and categorical data ordinal encoding\n",
        "x = train_df[numerical_columns]\n",
        "x[['X', 'Y']] = minmax.fit_transform(x[['X', 'Y']])\n",
        "\n",
        "y = train_df[target]\n",
        "y = np.array(y)\n",
        "y = np.reshape(y, (-1, 1))\n",
        "y = ordinal_encoder.fit_transform(y)\n",
        "\n",
        "# split data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10, stratify=y)\n",
        "log_cat_model.fit(x_train, y_train)\n",
        "\n",
        "# show percentage of correct results\n",
        "print('>>> log loss coefficient = ' + str(log_loss(y_test, log_cat_model.predict_proba(x_test))))\n",
        "# >>> log loss coefficient = 2.592240271616436"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> log loss coefficient = 2.592240271616436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEBfPAeu6jBd"
      },
      "source": [
        "# Random forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwq-tnOm_3kw"
      },
      "source": [
        "## RFC minmaxXY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHpVxZhXtzP3"
      },
      "source": [
        "**THIS ONE HAS THE BEST RESULT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLu6Racx_wMy"
      },
      "source": [
        "rfc = RandomForestClassifier(max_depth = 15, min_samples_leaf = 6, max_features = 'auto', \n",
        "                             min_samples_split = 4, min_weight_fraction_leaf = 0.0, n_estimators = 700, \n",
        "                             n_jobs = -1, random_state = 42, verbose = 2)\n",
        "    \n",
        "x = train_df[numerical_columns]\n",
        "x[['X', 'Y']] = minmax.fit_transform(x[['X', 'Y']])\n",
        "\n",
        "y = train_df[target]\n",
        "y = np.array(y)\n",
        "y = np.reshape(y, (-1, 1))\n",
        "y = ordinal_encoder.fit_transform(y)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10, stratify=y)\n",
        "rfc.fit(x_train, y_train)\n",
        "\n",
        "# current code execution out:\n",
        "# [Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed: 13.2min finished\n",
        "\n",
        "# RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
        "#                        criterion='gini', max_depth=15, max_features='auto',\n",
        "#                        max_leaf_nodes=None, max_samples=None,\n",
        "#                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "#                        min_samples_leaf=6, min_samples_split=4,\n",
        "#                        min_weight_fraction_leaf=0.0, n_estimators=700,\n",
        "#                        n_jobs=-1, oob_score=False, random_state=42, verbose=2,\n",
        "#                        warm_start=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj1zkJCT_1Qx",
        "outputId": "8920f1cb-34c8-497f-8898-b9648fd263a3"
      },
      "source": [
        "# show log loss\n",
        "print('>>> log loss coefficient = ' + str(log_loss(y_test, rfc.predict_proba(x_test))))\n",
        "# >>> log loss coefficient = 2.320314742290441"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    1.7s\n",
            "[Parallel(n_jobs=2)]: Done 158 tasks      | elapsed:    7.1s\n",
            "[Parallel(n_jobs=2)]: Done 361 tasks      | elapsed:   16.1s\n",
            "[Parallel(n_jobs=2)]: Done 644 tasks      | elapsed:   28.5s\n",
            "[Parallel(n_jobs=2)]: Done 700 out of 700 | elapsed:   31.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">>> log loss coefficient = 2.320314742290441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUlFn_6DAlQA"
      },
      "source": [
        "## RFC minmaxXY, standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSaLB66g6jBd"
      },
      "source": [
        "st_rfc = RandomForestClassifier(max_depth = 15, min_samples_leaf = 6, max_features = 'auto', \n",
        "                             min_samples_split = 4, min_weight_fraction_leaf = 0.0, n_estimators = 700, \n",
        "                             n_jobs = -1, random_state = 42, verbose = 2)\n",
        "                        \n",
        "x = train_df[numerical_columns]\n",
        "x[['X', 'Y']] = minmax.fit_transform(x[['X', 'Y']])\n",
        "x = standard_scaler.fit_transform(x)\n",
        "\n",
        "y = train_df[target]\n",
        "y = np.array(y)\n",
        "y = np.reshape(y, (-1, 1))\n",
        "y = ordinal_encoder.fit_transform(y)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10, stratify=y)\n",
        "st_rfc.fit(x_train, y_train)\n",
        "\n",
        "# current code execution out:\n",
        "# [Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed: 13.8min finished\n",
        "\n",
        "# RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
        "#                        criterion='gini', max_depth=15, max_features='auto',\n",
        "#                        max_leaf_nodes=None, max_samples=None,\n",
        "#                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "#                        min_samples_leaf=6, min_samples_split=4,\n",
        "#                        min_weight_fraction_leaf=0.0, n_estimators=700,\n",
        "#                        n_jobs=-1, oob_score=False, random_state=42, verbose=2,\n",
        "#                        warm_start=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paa_SL9v6jBe",
        "outputId": "55a66b9f-5891-4441-9b4e-8842f4deff0d"
      },
      "source": [
        "# show log loss\n",
        "print('>>> log loss coefficient = ' + str(log_loss(y_test, st_rfc.predict_proba(x_test))))\n",
        "# >>> log loss coefficient = 2.3204917991506244"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    1.5s\n",
            "[Parallel(n_jobs=2)]: Done 158 tasks      | elapsed:    6.4s\n",
            "[Parallel(n_jobs=2)]: Done 361 tasks      | elapsed:   14.2s\n",
            "[Parallel(n_jobs=2)]: Done 644 tasks      | elapsed:   25.1s\n",
            "[Parallel(n_jobs=2)]: Done 700 out of 700 | elapsed:   27.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">>> log loss coefficient = 2.3204917991506244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-_XE28R6jBh"
      },
      "source": [
        "# Light GBM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrIqpCd36jBi"
      },
      "source": [
        "## LGBM numerical + categorical, minmaxXY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V19sEzETB6KB",
        "outputId": "da395f47-6347-4322-e8f5-328ed210dde7"
      },
      "source": [
        "x = train_df[numerical_columns + categorical_columns]\n",
        "x[categorical_columns] = ordinal_encoder.fit_transform(x[categorical_columns])\n",
        "x[['X', 'Y']] = minmax.fit_transform(x[['X', 'Y']])\n",
        "\n",
        "y = train_df[target]\n",
        "y = np.array(y)\n",
        "y = np.reshape(y, (-1, 1))\n",
        "y = ordinal_encoder.fit_transform(y)\n",
        "y = y.flat\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10, stratify=y)\n",
        "train_data = lgb.Dataset(x_train, label=y_train)\n",
        "\n",
        "parameters = {'num_class': 39, 'objective': 'multiclass', 'metric': 'multi_logloss',\n",
        "              'device': 'CPU', 'boosting_type': 'gbdt', 'learning_rate': 0.03}\n",
        "num_rounds = 1000\n",
        "lgb_model = lgb.train(parameters, train_data, num_rounds)\n",
        "\n",
        "preds = lgb_model.predict(x_test)\n",
        "print('>>> log loss coefficient = ' + str(log_loss(y_test, preds)))\n",
        "# >>> log loss coefficient = 2.5086015150608554"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> log loss coefficient = 2.5086015150608554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyPJ6VVsVz74"
      },
      "source": [
        "## LGBM numerical, minmaxXY, standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHoCUWJggU11"
      },
      "source": [
        "**THIS VERSION SHOWS BEST RESULT FROM ALL MODELS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGcT1PGqVofX",
        "outputId": "be0b63a1-a509-4991-eafe-b448f7e5e1a6"
      },
      "source": [
        "x = train_df[numerical_columns]\n",
        "x[['X', 'Y']] = minmax.fit_transform(x[['X', 'Y']])\n",
        "x = standard_scaler.fit_transform(x)\n",
        "\n",
        "y = train_df[target]\n",
        "y = np.array(y)\n",
        "y = np.reshape(y, (-1, 1))\n",
        "y = ordinal_encoder.fit_transform(y)\n",
        "y = y.flat\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10, stratify=y)\n",
        "train_data = lgb.Dataset(x_train, label=y_train)\n",
        "\n",
        "parameters = {'num_class': 39, 'objective': 'multiclass', 'metric': 'multi_logloss',\n",
        "              'device': 'CPU', 'boosting_type': 'gbdt', 'learning_rate': 0.01,\n",
        "              'verbose': True\n",
        "            #   'gpu_platform_id': 1, 'gpu_device_id': 1\n",
        "              }\n",
        "num_rounds = 1500\n",
        "lgb_st_model = lgb.train(parameters, train_data, num_rounds)\n",
        "\n",
        "preds = lgb_st_model.predict(x_test)\n",
        "print('>>> log loss coefficient = ' + str(log_loss(y_test, preds)))\n",
        "# >>> log loss coefficient = 2.299090797046126"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> log loss coefficient = 2.2741577849846024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tv3NKJb352Z"
      },
      "source": [
        "## LGBM experimental features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4_TpoCy4Cxn"
      },
      "source": [
        "lgbm_experimental_columns = ['Month', 'DayOfYear', 'Hour', 'Minute', 'weekdayNumerical', \n",
        "                            'address_encoded', 'Season', 'Year', 'X', 'Y']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-1a9ofP4KXV",
        "outputId": "bfcf8460-0d03-482c-c893-57bb023ddee5"
      },
      "source": [
        "x = train_df[lgbm_experimental_columns]\n",
        "\n",
        "y = train_df[target]\n",
        "y = np.array(y)\n",
        "y = np.reshape(y, (-1, 1))\n",
        "y = ordinal_encoder.fit_transform(y)\n",
        "y = y.flat\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10, stratify=y)\n",
        "train_data = lgb.Dataset(x_train, label=y_train)\n",
        "\n",
        "parameters = {'num_class': 39, 'objective': 'multiclass', 'metric': 'multi_logloss',\n",
        "              'device': 'CPU', 'boosting_type': 'gbdt', 'learning_rate': 0.03}\n",
        "num_rounds = 1000\n",
        "lgb_st_model = lgb.train(parameters, train_data, num_rounds)\n",
        "\n",
        "preds = lgb_st_model.predict(x_test)\n",
        "print('>>> log loss coefficient = ' + str(log_loss(y_test, preds)))\n",
        "# >>> log loss coefficient = 2.3044181128810974"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> log loss coefficient = 2.3044181128810974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hqd08_RFSWI"
      },
      "source": [
        "## LGBM experimental, standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqyYmTDFEF9o",
        "outputId": "6607eb49-a70a-4153-de64-9132c3e7b310"
      },
      "source": [
        "x = train_df[lgbm_experimental_columns]\n",
        "x = standard_scaler.fit_transform(x)\n",
        "\n",
        "y = train_df[target]\n",
        "y = np.array(y)\n",
        "y = np.reshape(y, (-1, 1))\n",
        "y = ordinal_encoder.fit_transform(y)\n",
        "y = y.flat\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10, stratify=y)\n",
        "train_data = lgb.Dataset(x_train, label=y_train)\n",
        "\n",
        "parameters = {'num_class': 39, 'objective': 'multiclass', 'metric': 'multi_logloss',\n",
        "              'device': 'CPU', 'boosting_type': 'gbdt', 'learning_rate': 0.03}\n",
        "num_rounds = 1000\n",
        "lgb_st_exp_model = lgb.train(parameters, train_data, num_rounds)\n",
        "\n",
        "exp_preds = lgb_st_exp_model.predict(x_test)\n",
        "print('>>> log loss coefficient = ' + str(log_loss(y_test, exp_preds)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> log loss coefficient = 2.3183813338173564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frM36urlDY7K"
      },
      "source": [
        "# K-nearest neighbor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UupJRtne6jBm"
      },
      "source": [
        "## KNN numerical minmaxXY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBa1Fd8I6jBm",
        "outputId": "6f8b8d68-a545-4a18-ac5e-d3130877cab7"
      },
      "source": [
        "# lower amount of columns considering hard computation time of knn\n",
        "knn_numerical_columns = ['address_encoded', 'Year', 'X', 'Y', 'harmonicHourX', 'harmonicHourY', \n",
        "                         'harmonicWeekdayX', 'harmonicWeekdayY', 'harmonicMonthX', 'harmonicMonthY', \n",
        "                        #  'harmonicMinuteX', 'harmonicMinuteY']\n",
        "                        ]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.94 ms (started: 2021-08-03 15:24:05 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uU2BFAkn6jBm",
        "outputId": "8271e846-3a20-4ae2-9954-fe73a8353f7a"
      },
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=1000, metric='euclidean')\n",
        "\n",
        "x = train_df[knn_numerical_columns]\n",
        "x[['X', 'Y']] = minmax.fit_transform(x[['X', 'Y']])\n",
        "\n",
        "y = train_df[target]\n",
        "y = np.array(y)\n",
        "y = np.reshape(y, (-1, 1))\n",
        "y = ordinal_encoder.fit_transform(y)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10, stratify=y)\n",
        "\n",
        "knn.fit(x_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=1000, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oveE1KVy6jBo"
      },
      "source": [
        "predictions = knn.predict_proba(x_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8FVZXZa6jBo",
        "outputId": "4b185f75-7da3-45c9-a75b-05a7e87c08a2"
      },
      "source": [
        "print('>>> log loss coefficient = ' + str(log_loss(y_test, predictions)))\n",
        "# >>> log loss coefficient = 2.6214143842491984"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> log loss coefficient = 2.6214143842491984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqzooJJS6jBo"
      },
      "source": [
        "## KNN numerical minmaxXY, standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdauf_Fh6jBo",
        "outputId": "7d1976f0-f8ee-4a20-baa0-e381fdc1a0fd"
      },
      "source": [
        "knn_st = KNeighborsClassifier(n_neighbors=1000, metric='euclidean')\n",
        "\n",
        "x = train_df[knn_numerical_columns]\n",
        "x[['X', 'Y']] = minmax.fit_transform(x[['X', 'Y']])\n",
        "x = standard_scaler.fit_transform(x)\n",
        "\n",
        "y = train_df[target]\n",
        "y = np.array(y)\n",
        "y = np.reshape(y, (-1, 1))\n",
        "y = ordinal_encoder.fit_transform(y)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10, stratify=y)\n",
        "\n",
        "knn_st.fit(x_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=1000, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLS8ze426jBo"
      },
      "source": [
        "st_predictions = knn_st.predict_proba(x_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlSmhKSD6jBp",
        "outputId": "155f6a9a-056e-4d03-c5e6-eee47c84fa3d"
      },
      "source": [
        "print('>>> log loss coefficient = ' + str(log_loss(y_test, st_predictions)))\n",
        "# >>> log loss coefficient = 2.598232639007905"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> log loss coefficient = 2.598232639007905\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9g1CQTb6jBp"
      },
      "source": [
        "## KNN Manhattan numerical, minmaxXY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EDZnPBi6jBp",
        "outputId": "ce5e1f99-33c4-42bd-8f6f-cc07443a20c8"
      },
      "source": [
        "knn_man = KNeighborsClassifier(n_neighbors=1000, metric='manhattan')\n",
        "\n",
        "x = train_df[knn_numerical_columns]\n",
        "x[['X', 'Y']] = minmax.fit_transform(x[['X', 'Y']])\n",
        "\n",
        "y = train_df[target]\n",
        "y = np.array(y)\n",
        "y = np.reshape(y, (-1, 1))\n",
        "y = ordinal_encoder.fit_transform(y)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10, stratify=y)\n",
        "\n",
        "knn_man.fit(x_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=1000, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMP0eSjj6jBp"
      },
      "source": [
        "man_predictions = knn_man.predict_proba(x_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np_c7BMS6jBp",
        "outputId": "6fdd9dec-11fa-424d-fdfa-466124040abd"
      },
      "source": [
        "print('>>> log loss coefficient = ' + str(log_loss(y_test, man_predictions)))\n",
        "# >>> log loss coefficient = 2.593904040150504"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> log loss coefficient = 2.593904040150504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NubGCU9c6jBw"
      },
      "source": [
        "## KNN Manhattan numerical minmaxXY, standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ0wujH47jzU"
      },
      "source": [
        "**THIS MODEL HAS THE BEST RESULT IN KNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "V-nt1eNs6jBx",
        "outputId": "7a714959-45b9-4f42-f662-0cf2a73fcff4"
      },
      "source": [
        "knn_man_st = KNeighborsClassifier(n_neighbors=2000, metric='manhattan')\n",
        "\n",
        "x = train_df[numerical_columns]\n",
        "x[['X', 'Y']] = minmax.fit_transform(x[['X', 'Y']])\n",
        "x = standard_scaler.fit_transform(x)\n",
        "\n",
        "y = train_df[target]\n",
        "y = np.array(y)\n",
        "y = np.reshape(y, (-1, 1))\n",
        "y = ordinal_encoder.fit_transform(y)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10, stratify=y)\n",
        "\n",
        "knn_man_st.fit(x_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cca5b132ebc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mknn_man_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'manhattan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumerical_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandard_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'KNeighborsClassifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNjYTJuA6jBx"
      },
      "source": [
        "man_st_predictions = knn_man_st.predict_proba(x_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZjamywB6jBx"
      },
      "source": [
        "print('>>> log loss coefficient = ' + str(log_loss(y_test, man_st_predictions)))\n",
        "# >>> log loss coefficient = 2.55 (this was with 1000 neighbors)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}